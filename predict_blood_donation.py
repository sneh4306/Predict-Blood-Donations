# -*- coding: utf-8 -*-
"""predict-blood-donation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hxSziO-WLoKHnnNJhEcl5ZNjmRFCukE0

## 1. Inspecting Transfusion.data file
"""

# Print out the first 5 lines from the transfusion.data file
#system('head -n 5 datasets/transfusion.data')

"""## 2. Load the blood donations dataset"""

import pandas as pd
import numpy as np

transfusion = pd.read_csv('datasets/transfusion.data')

transfusion.head()

"""## 3. Get summary of blood donation dataset"""

transfusion.info()

"""## 4. Creating target column"""

transfusion.rename(columns={'whether he/she donated blood in March 2007':'target'},inplace=True)

transfusion.head(2)

"""## 5. Checking target incidence  
We want to predict whether or not the same donor will give blood the next time the vehicle comes to campus.   
The model for this is a binary classifier, meaning that there are only 2 possible outcomes:  
0 - the donor will not give blood  
1 - the donor will give blood  
Target incidence is defined as the number of cases of each individual target value in a dataset. That is, how many 0s in the target column compared to how many 1s?  
Target incidence gives us an idea of how balanced (or imbalanced) is our dataset.
"""

transfusion.target.value_counts(normalize=True).round(3)

"""## 6. Splitting dataset into train and test datasets"""

from sklearn.model_selection import train_test_split

# Split transfusion DataFrame into
# X_train, X_test, y_train and y_test datasets,
# stratifying on the `target` column
X_train, X_test, y_train, y_test = train_test_split(
    transfusion.drop(columns='target'),
    transfusion.target,
    test_size=0.25,
    random_state=42,
    stratify=transfusion.target
)
X_train.head(2)

"""## 7. Selecting model using TPOT"""

# Import TPOTClassifier and roc_auc_score
from tpot import TPOTClassifier
from sklearn.metrics import roc_auc_score

# Instantiate TPOTClassifier
tpot = TPOTClassifier(
    generations=5,
    population_size=20,
    verbosity=2,
    scoring='roc_auc',
    random_state=42,
    disable_update_check=True,
    config_dict='TPOT light'
)
tpot.fit(X_train, y_train)

# AUC score for tpot model
tpot_auc_score = roc_auc_score(y_test, tpot.predict_proba(X_test)[:, 1])
print(f'\nAUC score: {tpot_auc_score:.4f}')

# Print best pipeline steps
print('\nBest pipeline steps:', end='\n')
for idx, (name, transform) in enumerate(tpot.fitted_pipeline_.steps, start=1):
    # Print idx and transform
    print(f'{idx}. {transform}')

"""## 8. Checking Variance"""

print(X_train.var().round(3))

"""## 9. Log Normalization"""

# Copy X_train and X_test into X_train_normed and X_test_normed
X_train_normed, X_test_normed = X_train.copy(), X_test.copy()

# Specify which column to normalize
col_to_normalize = X_train_normed.var().idxmax(axis=1)

# Log normalization
for df_ in [X_train_normed, X_test_normed]:
    # Add log normalized column
    df_['monetary_log'] = np.log(df_[col_to_normalize])
    # Drop the original column
    df_.drop(columns=col_to_normalize, inplace=True)

# Check the variance for X_train_normed
print(X_train_normed.var().round(3))

"""## 10. Training Logistic Regression model"""

from sklearn import linear_model

# Instantiate LogisticRegression
logreg = linear_model.LogisticRegression(
    solver='liblinear',
    random_state=42
)

# Train the model
logreg.fit(X_train_normed, y_train)

# AUC score for tpot model
logreg_auc_score = roc_auc_score(y_test, logreg.predict_proba(X_test_normed)[:, 1])
print(f'\nAUC score: {logreg_auc_score:.4f}')

"""## 11. Conclusion  
In this notebook, we explored automatic model selection using TPOT and AUC score we got was 0.7850.   
This is better than simply choosing 0 all the time (the target incidence suggests that such a model would have 76% success rate).   
We then log normalized our training data and improved the AUC score by 0.5%. In the field of machine learning, even small improvements in accuracy can be important, depending on the purpose.  
Another benefit of using logistic regression model is that it is interpretable. We can analyze how much of the variance in the response variable (target) can be explained by other variables in our dataset.
"""

from operator import itemgetter

# Sort models based on their AUC score from highest to lowest
sorted(
    [('tpot', tpot_auc_score.round(4)), ('logreg', logreg_auc_score.round(4))],
    key=itemgetter(1),
    reverse=True
)

